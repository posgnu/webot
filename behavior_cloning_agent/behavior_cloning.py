import gzip
from lib2to3.pgen2 import token
import logging
import os
import pickle
import random

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.vocab import build_vocab_from_iterator

import computergym
import gym

from utils import (
    available_actions,
    data_transform,
    DATA_DIR,
    DATA_FILE,
    MODEL_FILE,
    QUATIZATION_SIZE,
    VOCAB_FILE,
)

from model import Net


class BehaviorCloning:
    def __init__(
        self,
        env_name: str = "click-button",
        epochs: int = 30,
        batch_size: int = 32,
        train_val_split: float = 0.85,
    ) -> None:
        self.dev = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.epochs = epochs
        self.batch_size = batch_size
        self.train_val_split = train_val_split
        self.env_name = env_name

    def build_model(self, vocab):
        self.m = Net(vocab, self.dev)
        self.m = self.m.to(self.dev)

    def restore(self):
        # save vocab
        vocab_path = os.path.join(DATA_DIR, self.env_name + "_" + VOCAB_FILE)
        logging.info(f"Restore from {vocab_path}")
        self.vocab = torch.load(vocab_path)

        self.m = Net(len(self.vocab))

        model_path = os.path.join(DATA_DIR, self.env_name + "_" + MODEL_FILE)
        logging.info(f"Restore from {model_path}")
        self.m.load_state_dict(torch.load(model_path))

    def build_vocab(self, utterances: list):
        def yield_tokens(data_iter):
            for utterance in data_iter:
                yield [utterance]

        self.vocab = build_vocab_from_iterator(
            yield_tokens(utterances), specials=["<unk>"]
        )
        self.vocab.set_default_index(self.vocab["<unk>"])

        # save vocab
        vocab_path = os.path.join(DATA_DIR, self.env_name + "_" + VOCAB_FILE)
        torch.save(self.vocab, vocab_path)

        return self.vocab

    def read_data(self):
        """Read the data generated by keyboard_agent.py"""
        with gzip.open(
            os.path.join(DATA_DIR, self.env_name + "_" + DATA_FILE), "rb"
        ) as f:
            data = pickle.load(f)

        # TODO balance dataset by multiplying rare events

        random.shuffle(data)

        # To numpy arrays
        states, actions = map(np.array, zip(*data))
        img_states = np.array(list(map(lambda x: x["img"], states)))
        utterance_states = np.array(list(map(lambda x: x["utterance"], states)))
        utterance_states = [x.replace(" ", "") for x in utterance_states]
        self.build_model(self.build_vocab(utterance_states))

        img_states = img_states[actions != None].astype("float")
        actions = actions[actions != None]

        actions = np.array(
            list(
                map(lambda x: np.array([x["type"], int(x["x"]), int(x["y"])]), actions)
            )
        )

        # Encode actions
        actions_encoded = np.full((len(actions), len(available_actions)), -1)

        # Quantize coordinate and shift up for utterance space
        # Clip coordinate between 0, 39
        actions_encoded[:, 1:] = actions[:, 1:]
        actions_encoded[:, 2] = actions_encoded[:, 2] - 50
        actions_encoded[:, 1:] = np.clip(
            actions_encoded[:, 1:] // QUATIZATION_SIZE, 0, 39
        )

        for i, a in enumerate(available_actions["action_type"]):
            actions_encoded[actions[:, 0] == a, 0] = i

        # Drop unsupported actions
        img_states = img_states[actions_encoded[:, 0] != -1]
        actions_encoded = actions_encoded[actions_encoded[:, 0] != -1]

        """
        for state in img_states[:10]:
            # Display training state images
            rgb = state.reshape((210, 160, 3)).astype("int")

            from PIL import Image

            img = Image.fromarray(rgb.astype("uint8"), "RGB")
            img.show()
        """

        for i, a in enumerate(available_actions["action_type"]):
            print(
                f"Actions of type {a}: {len(actions_encoded[actions_encoded[:, 0] == i])}"
            )

        print(f"Total transitions: {str(len(actions_encoded))}")

        # Process image
        img_states = np.array([state.reshape((210, 160, 3)) for state in img_states])
        img_states = np.moveaxis(img_states, 3, 1)  # channel first (torch requirement)

        return list(zip(img_states, utterance_states)), actions_encoded

    def create_datasets(self):
        """Create training and validation datasets"""

        class DatasetTransforms(torch.utils.data.Dataset):
            """
            Helper class to allow transformations
            by default TensorDataset doesn't support them
            """

            def __init__(self, x, y):
                self.x = x
                self.y = y

            def __len__(self):
                assert len(self.x) == len(self.y)
                return len(self.x)

            def __getitem__(self, index):
                img_tensor = data_transform(torch.tensor(self.x[index][0]))

                return (img_tensor, self.x[index][1], self.y[index])

        x, y = self.read_data()

        # train dataset
        x_train = x[: int(len(x) * self.train_val_split)]
        y_train = y[: int(len(y) * self.train_val_split)]

        train_set = DatasetTransforms(x_train, torch.tensor(y_train))

        train_loader = torch.utils.data.DataLoader(
            train_set,
            batch_size=self.batch_size,
            shuffle=True,
        )
        # num_workers=2)

        # test dataset
        x_val, y_val = x[int(len(x_train)) :], y[int(len(y_train)) :]

        val_set = DatasetTransforms(x_val, torch.tensor(y_val))

        val_loader = torch.utils.data.DataLoader(
            val_set,
            batch_size=self.batch_size,
            shuffle=False,
        )
        # num_workers=2)

        return train_loader, val_loader

    def train_model(self, train_loader, val_order):
        """
        Training main method
        :param model: the network
        :param device: the cuda device
        """

        loss_function = nn.CrossEntropyLoss()

        optimizer = optim.Adam(self.m.parameters())

        # train
        for epoch in range(self.epochs):
            print("Epoch {}/{}".format(epoch + 1, self.epochs))

            self.train_epoch(
                loss_function,
                optimizer,
                train_loader,
            )

            self.test(loss_function, val_order)

            # save model
            model_path = os.path.join(DATA_DIR, self.env_name + "_" + MODEL_FILE)
            torch.save(self.m.state_dict(), model_path)

    def train_epoch(
        self,
        loss_function,
        optimizer,
        data_loader,
    ):
        """Train for a single epoch"""

        # set model to training mode
        self.m.train()

        current_loss = 0.0
        current_acc = 0

        # iterate over the training data
        for i, (inputs, utterances, labels) in enumerate(data_loader):
            # send the input/labels to the GPU
            inputs = inputs.to(self.dev)
            labels = labels.to(self.dev)

            # zero the parameter gradients
            # optimizer.zero_grad()

            with torch.set_grad_enabled(True):
                # forward
                action_type, x_coordinate, y_coordinate = self.m(inputs, utterances)
                type_predictions = torch.argmax(action_type, 1)
                x_predictions = torch.argmax(x_coordinate, 1)
                y_predictions = torch.argmax(y_coordinate, 1)

                type_loss = loss_function(action_type, labels[:, 0])
                x_loss = loss_function(x_coordinate, labels[:, 1])
                y_loss = loss_function(y_coordinate, labels[:, 2])

                loss = type_loss + x_loss + y_loss

                # backward
                loss.backward()
                optimizer.step()

            # statistics
            current_loss += loss.item() * inputs.size(0)
            for type_pred, x_pred, y_pred, label in zip(
                type_predictions, x_predictions, y_predictions, labels
            ):
                if label[0] == type_pred and label[1] == x_pred and label[2] == y_pred:
                    current_acc += 1

        total_loss = current_loss / len(data_loader.dataset)
        total_acc = current_acc / len(data_loader.dataset)

        print("Train Loss: {:.4f}; Accuracy: {:.4f}".format(total_loss, total_acc))

    def test(self, loss_function, data_loader):
        """Test over the whole dataset"""

        self.m.eval()  # set model in evaluation mode

        current_loss = 0.0
        current_acc = 0

        # iterate over the validation data
        for i, (inputs, utterances, labels) in enumerate(data_loader):
            # send the input/labels to the GPU
            inputs = inputs.to(self.dev)
            labels = labels.to(self.dev)

            # forward
            with torch.set_grad_enabled(False):
                # here output should not be manually separated
                action_type, x_coordinate, y_coordinate = self.m(inputs, utterances)
                type_predictions = torch.argmax(action_type, 1)
                x_predictions = torch.argmax(x_coordinate, 1)
                y_predictions = torch.argmax(y_coordinate, 1)

                type_loss = loss_function(action_type, labels[:, 0])
                x_loss = loss_function(x_coordinate, labels[:, 1])
                y_loss = loss_function(y_coordinate, labels[:, 2])

                loss = type_loss + x_loss + y_loss

            # statistics
            current_loss += loss.item() * inputs.size(0)
            for type_pred, x_pred, y_pred, label in zip(
                type_predictions, x_predictions, y_predictions, labels
            ):
                if label[0] == type_pred and label[1] == x_pred and label[2] == y_pred:
                    current_acc += 1

        total_loss = current_loss / len(data_loader.dataset)
        total_acc = current_acc / len(data_loader.dataset)

        print("Test Loss: {:.4f}; Accuracy: {:.4f}".format(total_loss, total_acc))

    def play(self, episodes: int = 10):
        """
        Let the agent play
        :param model: the network
        :param device: the cuda device
        """
        self.m.eval()
        self.m = self.m.to(self.dev)

        env = gym.make("MiniWoBEnv-v0", env_name=self.env_name)

        for _ in range(episodes):
            # initialize environment
            states = env.reset(seeds=[random.random()], record_screenshots=True)

            done = False
            while not done:
                img_states = [x.screenshot for x in states]

                # FIXME: state dtype should be float. otherwise self.preprocess output differnet result
                img_state = np.moveaxis(
                    np.array(img_states[0]).astype("float"), 2, 0
                )  # channel first image
                img_state = self.img_preprocess(img_state)

                goal_state = torch.tensor(
                    self.vocab(self.tokenizer(states[0].utterance))
                )

                # forward
                # FIXME do not manually separate output
                with torch.set_grad_enabled(False):
                    action_types, x_coordinates, y_coordinates = self.m(
                        img_state, [goal_state]
                    )

                action_type, x_coordinate, y_coordinate = (
                    action_types[0],
                    x_coordinates[0],
                    y_coordinates[0],
                )
                type_prediction = torch.argmax(action_type)
                x_prediction = torch.argmax(x_coordinate)
                y_prediction = torch.argmax(y_coordinate)

                # translate from net output to env action
                x_coordinate = x_prediction * QUATIZATION_SIZE + (QUATIZATION_SIZE / 2)
                y_coordinate = (
                    y_prediction * QUATIZATION_SIZE + (QUATIZATION_SIZE / 2) + 50
                )

                action = [type_prediction, int(x_coordinate), int(y_coordinate)]
                print(
                    f"{available_actions['action_type'][action[0]]} at ({action[1]}, {action[2]})"
                )
                import time

                time.sleep(2)

                states, _, dones, _ = env.step([action])
                done = all(dones)

        env.close()

    def img_preprocess(self, state):
        # numpy to tensor
        # Why this flip happens?
        state = torch.from_numpy(np.flip(state, axis=0).copy())
        state = data_transform(state)  # apply transformations
        state = state.unsqueeze(0)  # add additional dimension
        state = state.to(self.dev)  # transfer to GPU
        return state


if __name__ == "__main__":
    bc_agent = BehaviorCloning(env_name="click-button")
    bc_agent.restore()
    # train_loader, val_order = self.create_datasets()
    # bc_agent.train_model(train_loader, val_order)

    bc_agent.play(50)
