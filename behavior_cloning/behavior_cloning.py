import gzip
import os
import pickle
import random

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

import computergym
import gym

from utils import (
    available_actions,
    data_transform,
    DATA_DIR,
    DATA_FILE,
    MODEL_FILE,
    build_network,
)


class BehaviorCloning:
    def __init__(
        self,
        env_name: str = "click-button",
        epochs: int = 30,
        batch_size: int = 32,
        train_val_split: float = 0.85,
    ) -> None:
        self.dev = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.m = build_network()
        self.epochs = epochs
        self.batch_size = batch_size
        self.train_val_split = train_val_split
        self.env_name = env_name

    def train(self):
        self.m.eval()
        self.m = self.m.to(self.dev)
        self.train_model(self.m, self.dev)

    def restore(self):
        model_path = os.path.join(DATA_DIR, MODEL_FILE)
        self.m.load_state_dict(torch.load(model_path))

    def read_data(self):
        """Read the data generated by keyboard_agent.py"""
        with gzip.open(os.path.join(DATA_DIR, DATA_FILE), "rb") as f:
            data = pickle.load(f)

        # TODO balance dataset by multiplying rare events

        random.shuffle(data)

        # To numpy arrays
        actions, states = map(np.array, zip(*data))
        states = states[actions != None].astype("float")
        actions = actions[actions != None]
        actions = np.array(
            list(map(lambda x: np.array([x["type"], x["x"], x["y"]]), actions))
        )
        """
        for state in states:
            # Display training state images
            rgb = state.reshape((210, 160, 3))[50:, ...].astype("int")
            import matplotlib.pyplot as plt

            print(rgb.shape)
            plt.axis("off")
            plt.imshow(rgb)
            plt.show()
            break
        """
        # Encode actions
        actions_encoded = np.full((len(actions), len(available_actions)), -1)
        actions_encoded[:, 1:] = actions[:, 1:]

        for i, a in enumerate(available_actions[0]):
            actions_encoded[actions[:, 0] == a, 0] = i

        # Drop unsupported actions
        states = states[actions_encoded[:, 0] != -1]
        actions_encoded = actions_encoded[actions_encoded[:, 0] != -1]

        for i, a in enumerate(available_actions[0]):
            print(
                f"Actions of type {a}: {len(actions_encoded[actions_encoded[:, 0] == i])}"
            )

        print(f"Total transitions: {str(len(actions_encoded))}")

        return states, actions_encoded

    def create_datasets(self):
        """Create training and validation datasets"""

        class TensorDatasetTransforms(torch.utils.data.TensorDataset):
            """
            Helper class to allow transformations
            by default TensorDataset doesn't support them
            """

            def __init__(self, x, y):
                super().__init__(x, y)

            def __getitem__(self, index):
                tensor = data_transform(self.tensors[0][index])
                return (tensor,) + tuple(t[index] for t in self.tensors[1:])

        x, y = self.read_data()
        x = np.array([state.reshape((210, 160, 3))[50:, ...] for state in x])
        x = np.moveaxis(x, 3, 1)  # channel first (torch requirement)

        # train dataset
        x_train = x[: int(len(x) * self.train_val_split)]
        y_train = y[: int(len(y) * self.train_val_split)]

        train_set = TensorDatasetTransforms(
            torch.tensor(x_train), torch.tensor(y_train)
        )

        train_loader = torch.utils.data.DataLoader(
            train_set,
            batch_size=self.batch_size,
            shuffle=True,
        )
        # num_workers=2)

        # test dataset
        x_val, y_val = x[int(len(x_train)) :], y[int(len(y_train)) :]

        val_set = TensorDatasetTransforms(torch.tensor(x_val), torch.tensor(y_val))

        val_loader = torch.utils.data.DataLoader(
            val_set,
            batch_size=self.batch_size,
            shuffle=False,
        )
        # num_workers=2)

        return train_loader, val_loader

    def train_model(self, model, device):
        """
        Training main method
        :param model: the network
        :param device: the cuda device
        """

        type_loss_function = nn.CrossEntropyLoss()
        coordinate_loss_function = nn.MSELoss()

        optimizer = optim.Adam(model.parameters())

        train_loader, val_order = self.create_datasets()  # read datasets

        # train
        for epoch in range(self.epochs):
            print("Epoch {}/{}".format(epoch + 1, self.epochs))

            self.train_epoch(
                model,
                device,
                type_loss_function,
                coordinate_loss_function,
                optimizer,
                train_loader,
            )

            self.test(
                model, device, type_loss_function, coordinate_loss_function, val_order
            )

            # save model
            model_path = os.path.join(DATA_DIR, MODEL_FILE)
            torch.save(model.state_dict(), model_path)

    def train_epoch(
        self,
        model,
        device,
        type_loss_function,
        coordinate_loss_function,
        optimizer,
        data_loader,
    ):
        """Train for a single epoch"""

        # set model to training mode
        model.train()

        current_loss = 0.0
        current_acc = 0

        # iterate over the training data
        for i, (inputs, labels) in enumerate(data_loader):
            # send the input/labels to the GPU
            inputs = inputs.to(device)
            labels = labels.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            with torch.set_grad_enabled(True):
                # forward
                outputs = model(inputs)
                type_predictions = torch.argmax(outputs[:, :2], 1)
                coordinate_predictions = outputs[:, 2:]

                type_loss = type_loss_function(outputs[:, :2], labels[:, 0])
                labels = labels.to(torch.float32)
                coordinate_loss = coordinate_loss_function(
                    outputs[:, 2:], labels[:, 1:]
                )
                loss = type_loss + coordinate_loss

                # backward
                loss.backward()
                optimizer.step()

            # statistics
            current_loss += loss.item() * inputs.size(0)
            for type_pred, coord_pred, label in zip(
                type_predictions, coordinate_predictions, labels
            ):
                if label[0] == type_pred and np.allclose(
                    coord_pred.detach().numpy(), label[1:], atol=10
                ):
                    current_acc += 1

        total_loss = current_loss / len(data_loader.dataset)
        total_acc = current_acc / len(data_loader.dataset)

        print("Train Loss: {:.4f}; Accuracy: {:.4f}".format(total_loss, total_acc))

    def test(
        self, model, device, type_loss_function, coordinate_loss_function, data_loader
    ):
        """Test over the whole dataset"""

        model.eval()  # set model in evaluation mode

        current_loss = 0.0
        current_acc = 0

        # iterate over the validation data
        for i, (inputs, labels) in enumerate(data_loader):
            # send the input/labels to the GPU
            inputs = inputs.to(device)
            labels = labels.to(device)

            # forward
            with torch.set_grad_enabled(False):
                outputs = model(inputs)
                type_predictions = torch.argmax(outputs[:, :2], 1)
                coordinate_predictions = outputs[:, 2:]

                type_loss = type_loss_function(outputs[:, :2], labels[:, 0])
                labels = labels.to(torch.float32)
                coordinate_loss = coordinate_loss_function(
                    outputs[:, 2:], labels[:, 1:]
                )
                loss = type_loss + coordinate_loss

            # statistics
            current_loss += loss.item() * inputs.size(0)
            for type_pred, coord_pred, label in zip(
                type_predictions, coordinate_predictions, labels
            ):
                if label[0] == type_pred and np.allclose(
                    coord_pred.detach().numpy(), label[1:], atol=10
                ):
                    current_acc += 1

        total_loss = current_loss / len(data_loader.dataset)
        total_acc = current_acc / len(data_loader.dataset)

        print("Test Loss: {:.4f}; Accuracy: {:.4f}".format(total_loss, total_acc))

    def play(self, episodes: int = 10):
        """
        Let the agent play
        :param model: the network
        :param device: the cuda device
        """
        self.m.eval()
        self.m = self.m.to(self.dev)

        env = gym.make("MiniWoBEnv-v0", env_name=self.env_name)
        for _ in range(episodes):
            # initialize environment
            states = env.reset(seeds=[random.random()], record_screenshots=True)

            done = False
            while not done:
                state = np.moveaxis(np.array(states[0]), 2, 0)  # channel first image

                # numpy to tensor
                # Why this flip happens?
                state = torch.from_numpy(np.flip(state, axis=0).copy())
                state = data_transform(state)  # apply transformations
                state = state.unsqueeze(0)  # add additional dimension
                state = state.to(self.dev)  # transfer to GPU

                # forward
                with torch.set_grad_enabled(False):
                    outputs = self.m(state)

                outputs = outputs[0]
                normalized = torch.nn.functional.softmax(outputs[:2], dim=0)

                # translate from net output to env action
                action_type = np.argmax(normalized.cpu().numpy())
                x_coordinate = np.clip(outputs[2], 0, 159)
                y_coordinate = np.clip(outputs[3], 0, 159)

                action = [action_type, int(x_coordinate), int(y_coordinate)]
                print(
                    f"{available_actions[0][action_type]} at ({action[1]}, {action[2]})"
                )
                states, _, dones, _ = env.step([action])
                done = all(dones)

                import time

                time.sleep(1)
        env.close()


if __name__ == "__main__":
    bc_agent = BehaviorCloning()
    bc_agent.restore()
    # bc_agent.train()

    bc_agent.play()
